{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This file contains crawling codes to get cve info (including basic cwe info related to each cve) from https://www.cvedetails.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect urls for each CVE\n",
    "- all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling https://www.cvedetails.com/vulnerabilities-by-types.php: 100%|██████████| 399/399 [00:00<00:00, 256400.69it/s]\n",
      "crawling 'page' urls: 100%|██████████| 24/24 [00:12<00:00,  1.89it/s]\n",
      "collecting CVEs with urls: 100%|██████████| 3413/3413 [34:13<00:00,  1.66it/s] \n",
      " 48%|████▊     | 81744/169990 [00:00<00:00, 817401.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total CVE num: 169990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169990/169990 [00:00<00:00, 798636.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 CVE num: 863\n",
      "2000 CVE num: 1235\n",
      "2001 CVE num: 1538\n",
      "2002 CVE num: 2356\n",
      "2003 CVE num: 1500\n",
      "2004 CVE num: 2645\n",
      "2005 CVE num: 4626\n",
      "2006 CVE num: 6993\n",
      "2007 CVE num: 6459\n",
      "2008 CVE num: 7001\n",
      "2009 CVE num: 4905\n",
      "2010 CVE num: 5051\n",
      "2011 CVE num: 4605\n",
      "2012 CVE num: 5426\n",
      "2013 CVE num: 6146\n",
      "2014 CVE num: 8299\n",
      "2015 CVE num: 7935\n",
      "2016 CVE num: 9228\n",
      "2017 CVE num: 14451\n",
      "2018 CVE num: 15681\n",
      "2019 CVE num: 15429\n",
      "2020 CVE num: 17946\n",
      "2021 CVE num: 18120\n",
      "2022 CVE num: 1552\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Steps to crawl data from 'https://www.cvedetails.com/vulnerabilities-by-types.php'\n",
    "and construct our cyber KG.\n",
    "\n",
    "Step1 : Crawl each line of the top50 vendor table with following information\n",
    "        - 'vulner-link'  (str), \n",
    "\n",
    "Step2 : In the 'vulner-link', crawl A LIST of\n",
    "        - 'page-urls'    (str)\n",
    "        \n",
    "Step3:  Based on step2 'page-urls', crawl A DICT of\n",
    "        - 'cve-id':'cve-links'    (str: str),\n",
    "        \n",
    "Step4 : Based on step3, for each 'cve-link', we crawl following information:\n",
    "        - 'cve-desc'     (str)   # Descriptions\n",
    "        - 'pub-time'     (str)   # Publish time\n",
    "        - 'score'        (str)   # CVSS score\n",
    "        - 'vulner-types' (str)   # Vulnerability type\n",
    "        - 'cwe-link'     (str)\n",
    "        - 'product-info' (List[List[str]]) \n",
    "                          # List of ('vendor', 'product-name', 'version', 'product-type')\n",
    "        \n",
    "Step5: Based on step4, for the 'cwe-link', find\n",
    "        - 'cwe-id'       (str),\n",
    "        - 'cwe-desc'     (str),  # CWE descriptions\n",
    "        \n",
    "Step6: Based on step3,4,5, we construct a csv where each line contains following info\n",
    "        - 0th col: row index\n",
    "        - 1st col: 'cve-id'       (str)\n",
    "        - 2nd col: 'cve-link'     (str)\n",
    "        - 3rd col: 'cve-desc'     (str)\n",
    "        - 4th col: 'pub-time'     (str)\n",
    "        - 5th col: 'score'        (str)\n",
    "        - 6th col: 'vulner-types' (str,str,str)\n",
    "        - 7th col: 'cwe-id'       (str)\n",
    "        - 8th col: 'cwe-url'     (str)\n",
    "        - 9th col: 'cwe-def'      (str)  # replace textual description with CWE definition\n",
    "        - 10th col: 'cwe-rel'     (str,str,str;str,str,str;str,str,str)  # relevant CWE info\n",
    "                                  combination of 'relationships','cwe-id','cwe-name'\n",
    "        - 11th col: List of ('vendor', 'product-name', 'version', 'product-type'), (List[List[str]])\n",
    "                    but we save into string format: 'vendor,product-name,version,product-type;...'\n",
    "                    where ',' splits elements, ';' splits lines, no ',' or ';' at the beginning\n",
    "                    or end.\n",
    "        \n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "# root = 'https://www.cvedetails.com/top-50-vendors.php'\n",
    "root = 'https://www.cvedetails.com/vulnerabilities-by-types.php'\n",
    "url_save_dir = './cve_url/'  # save {cve: webpage url}\n",
    "os.makedirs(url_save_dir, exist_ok=True)\n",
    "\n",
    "response = requests.get(root)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# pages_to_cve = []   # len=50\n",
    "# for ele in tqdm(soup.find_all('a'), desc=f\"crawling {root}\"):\n",
    "#     if ele.has_attr('title') and \"All vulnerabilities related to products of\" in ele.get('title'):\n",
    "#         pages_to_cve.append(\"https://www.cvedetails.com\" + ele.get('href'))\n",
    "\n",
    "pages_to_cve = []\n",
    "for ele in tqdm(soup.find_all('a'), desc=f\"crawling {root}\"):\n",
    "    if ele.has_attr('href'):\n",
    "        href = ele.get('href')\n",
    "        if href.startswith('/vulnerability-list/year-') and href.endswith('/vulnerabilities.html'):\n",
    "            pages_to_cve.append(\"https://www.cvedetails.com\" + href)\n",
    "\n",
    "page_urls = []\n",
    "page_num = set()\n",
    "for url in tqdm(pages_to_cve, desc=f\"crawling 'page' urls\"):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') \n",
    "    for ele in soup.find_all('a'):\n",
    "        if ele.has_attr('title') and \"Go to page\" in ele.get('title'):\n",
    "            page_urls.append(\"https://www.cvedetails.com\" + ele.get('href'))\n",
    "            page_num.add(ele.text)\n",
    "\n",
    "# print(len(page_urls))\n",
    "\n",
    "all_urls = OrderedDict()  # 'cve-id': url\n",
    "for url in tqdm(page_urls, desc=f\"collecting CVEs with urls\"):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser') \n",
    "    for ele in soup.find_all('a'):\n",
    "        if ele.has_attr('title') and \"security vulnerability details\" in ele.get('title'):\n",
    "            cve_id = ele.text\n",
    "            if cve_id not in all_urls: # exactly one url for each cve-id\n",
    "                all_urls[cve_id] = \"https://www.cvedetails.com\" + ele.get('href')\n",
    "\n",
    "with open(os.path.join(url_save_dir, 'all_urls.json'), 'w') as f:  # TODO: save-path move into config\n",
    "    json.dump(all_urls, f)\n",
    "    print('total CVE num: %d' % len(all_urls))\n",
    "\n",
    "url_by_year = defaultdict(dict)\n",
    "for k, v in tqdm(all_urls.items()):\n",
    "    y = int(k.split('-')[1])\n",
    "    url_by_year[y].update({k: v})\n",
    "\n",
    "for y in sorted(list(url_by_year.keys())):\n",
    "    with open(os.path.join(url_save_dir, str(y)+'.json'), 'w') as f:  # TODO: save-path move into config\n",
    "        json.dump(url_by_year[y], f)\n",
    "        print('%d CVE num: %d' % (y, len(url_by_year[y])))\n",
    "print('Done')\n",
    "\n",
    "# NOTE: run this cell in jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl details from each CVE webpage\n",
    "- specify range of crawling years\n",
    "- run this part backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling CVE-2010 detailed info:   0%|          | 19/5050 [00:21<1:23:58,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://www.cvedetails.com/cve/CVE-2010-0206/ pd-info is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "crawling CVE-2010 detailed info:   0%|          | 20/5050 [00:22<1:15:25,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://www.cvedetails.com/cve/CVE-2010-0207/ pd-info is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling CVE-2010 detailed info:   0%|          | 23/5050 [00:25<1:22:17,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "https://www.cvedetails.com/cve/CVE-2010-0747/ pd-info is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "crawling CVE-2010 detailed info:   1%|          | 63/5050 [01:08<1:59:35,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "\n",
    "years = range(2010, 2022)  # adjustable\n",
    "url_load_dir = './cve_url/' # load {cve: webpage url}\n",
    "save_dir = '/data/zhaohan/adv-reasoning/data/cyberkg-raw/cve'\n",
    "\n",
    "for year in years:\n",
    "    cve_urls = OrderedDict()\n",
    "    with open(os.path.join(url_load_dir, str(year)+'.json'), 'r') as f:\n",
    "        cve_urls.update(json.load(f))\n",
    "    cve_urls = OrderedDict(reversed(list(cve_urls.items())))\n",
    "\n",
    "    cve_info = {'cve-id':      [],\n",
    "                'cve-url':     [], \n",
    "                'cve-desc':    [],\n",
    "                'pub-time':    [],\n",
    "                'score':       [],\n",
    "                'vulner-types':[],\n",
    "                'cwe-id':      [],\n",
    "                'cwe-url':     [],\n",
    "                'cwe-def':     [],\n",
    "                'cwe-rel':     [],\n",
    "                'pd-info':     [],\n",
    "               }\n",
    "    it, counter = 0, 1\n",
    "    for cve_id, url in tqdm(cve_urls.items(), desc=\"crawling CVE-%d detailed info\" % year):  \n",
    "        cve_info['cve-id'].append(cve_id)\n",
    "        cve_info['cve-url'].append(url)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # description\n",
    "        ele = soup.find('div', attrs={'class':'cvedetailssummary'})\n",
    "        if ele == None:\n",
    "            cve_info['cve-desc'].append('None')\n",
    "        else:\n",
    "            cve_desc = ele.get_text().split('\\n\\n')[0].strip()\n",
    "            cve_info['cve-desc'].append(cve_desc.replace('|', '<OR-OPERATOR>'))\n",
    "\n",
    "        # publish time\n",
    "        ele = soup.find('span', attrs={'class':'datenote'})\n",
    "        if ele == None:\n",
    "            cve_info['pub-time'].append('None')\n",
    "        else:\n",
    "            cve_info['pub-time'].append(ele.text.split('\\t')[1].split(':')[1].strip())\n",
    "\n",
    "        # CVSS score\n",
    "        ele = soup.find('div', attrs={'class':'cvssbox'})\n",
    "        try: cve_info['score'].append(ele.text)\n",
    "        except: \n",
    "            cve_info['score'].append('0.0')\n",
    "            print('\\n%s has no score' % (url))\n",
    "\n",
    "        # vulnerability type(s)\n",
    "        table = soup.find('table', attrs={'id':'cvssscorestable'})\n",
    "        if table==None:\n",
    "            print('\\n%s has no attribute table' % url)\n",
    "            cve_info['vulner-types'].append('None')\n",
    "        else:\n",
    "            for row in table.find_all('tr'):\n",
    "                if 'Vulnerability Type' in row.text:\n",
    "                    vul_types = []\n",
    "                    for vul_type in row.find_all('span'):\n",
    "                        vul_types.append(vul_type.text.strip())\n",
    "\n",
    "                    if len(vul_types)==0: # no type\n",
    "                        cve_info['vulner-types'].append('None')\n",
    "                    else:\n",
    "                        for _i in range(len(vul_types)):\n",
    "                            vul_types[_i].replace(',', '<comma>')\n",
    "                        cve_info['vulner-types'].append(','.join(vul_types))\n",
    "                    break\n",
    "\n",
    "        # # gained access\n",
    "        # for row in table.find_all('tr'):\n",
    "        #     if 'Gained Access' in row.text:\n",
    "        #         cve_info['gained-acs'].append(row.find('span').text)\n",
    "        #         break\n",
    "\n",
    "        # cwe info\n",
    "        if table == None:\n",
    "            cve_info['cwe-id'].append('None')\n",
    "            cve_info['cwe-url'].append('None')\n",
    "            cve_info['cwe-def'].append('None')\n",
    "            cve_info['cwe-rel'].append('None')\n",
    "            print('\\n%s has no attribute table' % url)\n",
    "\n",
    "        else:\n",
    "            for row in table.find_all('tr'):\n",
    "                if 'CWE ID' in row.text:\n",
    "                    if row.find('a') == None:\n",
    "                        cve_info['cwe-id'].append('None')\n",
    "                        cve_info['cwe-url'].append('None')\n",
    "                        cve_info['cwe-def'].append('None')\n",
    "                        cve_info['cwe-rel'].append('None')\n",
    "                        print('\\n%s not link to any CWE' % url)\n",
    "                        break\n",
    "\n",
    "                    cwe_url = 'https:' + row.find('a').get('href') \n",
    "                    cwe_id = row.find('a').text\n",
    "                    cwe_def, cwe_rel = 'None', 'None'\n",
    "\n",
    "                    cwe_html = BeautifulSoup(requests.get(cwe_url).content, 'html.parser')\n",
    "                    cwe_table = cwe_html.find('table', attrs={'class':'details'})\n",
    "\n",
    "                    # have url and a table\n",
    "                    if cwe_table != None:  \n",
    "\n",
    "                        # find cwe name\n",
    "                        if cwe_html.find('h1') != None:\n",
    "                            cwe_def = cwe_html.find('h1').text\n",
    "\n",
    "                        for cwe_row in cwe_table.find_all('tr'):\n",
    "                            if 'CWE Definition' in cwe_row.text:\n",
    "                                if cwe_row.find('a') == None or cwe_row.find('a').get('href') == None:\n",
    "                                    break\n",
    "                                cwe_detail_url = cwe_row.find('a').get('href')\n",
    "                                cwe_detail_html = BeautifulSoup(requests.get(cwe_detail_url).content, 'html.parser')\n",
    "\n",
    "                                # find cwe relations\n",
    "                                rel_table = cwe_detail_html.find('div', attrs={'id': 'Relationships'})\n",
    "                                if rel_table == None:\n",
    "                                    print('%s has no CWE relation table' % cwe_detail_url)\n",
    "                                    break\n",
    "                                    \n",
    "                                rel_cwe_details = set()\n",
    "                                for detail_row in rel_table.find_all('tr', attrs={'class':'primary Weakness'}):\n",
    "                                    row_text = detail_row.text\n",
    "                                    if not ('Nature' in row_text and 'Type' in row_text \\\n",
    "                                            and 'ID' in row_text and 'Name' in row_text):\n",
    "                                        texts = []\n",
    "                                        for col in detail_row.find_all('td'):\n",
    "                                            if col.text!=None:\n",
    "                                                texts.append(col.text)\n",
    "                                        texts[3] = texts[3].replace(',', '<comma>')\n",
    "                                        rel_cwe_details.add(','.join([texts[0], texts[2], texts[3]]))\n",
    "\n",
    "                                for detail_row in rel_table.find_all('tr', attrs={'class':'primary Category'}):\n",
    "                                    row_text = detail_row.text\n",
    "                                    if not ('Nature' in row_text and 'Type' in row_text \\\n",
    "                                            and 'ID' in row_text and 'Name' in row_text):\n",
    "                                        texts = []\n",
    "                                        for col in detail_row.find_all('td'):\n",
    "                                            if col.text!=None:\n",
    "                                                texts.append(col.text)\n",
    "                                        texts[3] = texts[3].replace(',', '<comma>')\n",
    "                                        rel_cwe_details.add(','.join([texts[0], texts[2], texts[3]]))\n",
    "\n",
    "                                if len(rel_cwe_details) > 0:\n",
    "                                    cwe_rel = ';'.join(rel_cwe_details)\n",
    "                                break\n",
    "\n",
    "                    cve_info['cwe-id'].append(cwe_id)\n",
    "                    cve_info['cwe-url'].append(cwe_url)\n",
    "                    cve_info['cwe-def'].append(cwe_def)\n",
    "                    cve_info['cwe-rel'].append(cwe_rel)\n",
    "                    break\n",
    "\n",
    "        # product info: List of ('vendor', 'product-name', 'version', 'product-type')\n",
    "        table = soup.find('table', attrs={'id':'vulnprodstable'})\n",
    "\n",
    "        try:\n",
    "            pd_info = []\n",
    "            for row in table.find_all('tr'):\n",
    "                if '#' in row.text and 'Vendor' in row.text and 'Update' in row.text:\n",
    "                    continue\n",
    "\n",
    "                cols = row.find_all('td')\n",
    "                pd_type = cols[1].text.strip().replace(',', '<comma>').replace(';', '<semicolon>')\n",
    "                pd_vendor = cols[2].text.strip().replace(',', '<comma>').replace(';', '<semicolon>')\n",
    "                pd_name = cols[3].text.strip().replace(',', '<comma>').replace(';', '<semicolon>')\n",
    "                pd_ver = cols[4].text.strip().replace(',', '<comma>').replace(';', '<semicolon>')\n",
    "\n",
    "                if pd_ver == '-':\n",
    "                    pd_ver = ''\n",
    "                pd_info.append(','.join([pd_type, pd_vendor, pd_name, pd_ver]))\n",
    "            pd_info = ';'.join(pd_info)\n",
    "\n",
    "            cve_info['pd-info'].append(pd_info)\n",
    "        except:\n",
    "            print('\\n%s pd-info is None' % url)\n",
    "            cve_info['pd-info'].append('None')\n",
    "\n",
    "        # turncate & save into csv file\n",
    "        if len(cve_info['cve-url']) == 5000 or it == len(cve_urls)-1:\n",
    "            df = pd.DataFrame.from_dict(cve_info)\n",
    "\n",
    "            save_path = os.path.join(save_dir, str(year))\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            df.to_csv(os.path.join(save_path, f'cve_{counter}.csv'), sep='|')\n",
    "\n",
    "            cve_info = {'cve-id':      [],\n",
    "                        'cve-url':     [], \n",
    "                        'cve-desc':    [],\n",
    "                        'pub-time':    [],\n",
    "                        'score':       [],\n",
    "                        'vulner-types':[],\n",
    "                        'cwe-id':      [],\n",
    "                        'cwe-url':     [],\n",
    "                        'cwe-def':     [],\n",
    "                        'cwe-rel':     [],\n",
    "                        'pd-info':     [],\n",
    "                       }\n",
    "            for k, v in cve_info.items():\n",
    "                assert len(v)==len(cve_info['cve-id']), '%s has len %d' % (k, len(v))\n",
    "\n",
    "            counter += 1\n",
    "        it += 1\n",
    "print(\"Done\")\n",
    "\n",
    "# NOTE: run this cell in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cve-id': [],\n",
       " 'cve-url': [],\n",
       " 'cve-desc': [],\n",
       " 'pub-time': [],\n",
       " 'score': [],\n",
       " 'vulner-types': [],\n",
       " 'cwe-id': [],\n",
       " 'cwe-url': [],\n",
       " 'cwe-def': [],\n",
       " 'cwe-rel': [],\n",
       " 'pd-info': ['Application,Siemens,Teamcenter Product Lifecycle Management,9.1.2.5']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "\n",
    "cve_info = {'cve-id':      [],\n",
    "            'cve-url':     [], \n",
    "            'cve-desc':    [],\n",
    "            'pub-time':    [],\n",
    "            'score':       [],\n",
    "            'vulner-types':[],\n",
    "            'cwe-id':      [],\n",
    "            'cwe-url':     [],\n",
    "            'cwe-def':     [],\n",
    "            'cwe-rel':     [],\n",
    "            'pd-info':     [],\n",
    "           }\n",
    "\n",
    "response = requests.get(\"https://www.cvedetails.com/cve/CVE-2018-11450/\")\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "table = soup.find('table', attrs={'id':'vulnprodstable'})\n",
    "\n",
    "try:\n",
    "    pd_info = []\n",
    "    for row in table.find_all('tr'):\n",
    "        if '#' in row.text and 'Vendor' in row.text and 'Update' in row.text:\n",
    "            continue\n",
    "\n",
    "        cols = row.find_all('td')\n",
    "        pd_type = cols[1].text.strip()\n",
    "        pd_vendor = cols[2].text.strip()\n",
    "        pd_name = cols[3].text.strip()\n",
    "        pd_ver = cols[4].text.strip()\n",
    "\n",
    "        if pd_ver == '-':\n",
    "            pd_ver = ''\n",
    "        pd_info.append(','.join([pd_type, pd_vendor, pd_name, pd_ver]))\n",
    "    pd_info = ';'.join(pd_info)\n",
    "\n",
    "    cve_info['pd-info'].append(pd_info)\n",
    "except:\n",
    "    print('\\n%s pd-info is None' % url)\n",
    "    cve_info['pd-info'].append('None')\n",
    "\n",
    "cve_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123;456'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =set()\n",
    "a.add('123')\n",
    "a.add('456')\n",
    "';'.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhaohan_conda_env",
   "language": "python",
   "name": "zhaohan_conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
